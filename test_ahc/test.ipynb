{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pulp\n",
    "import ast\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from requests.models import PreparedRequest\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator \n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from kneed import KneeLocator \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import pulp\n",
    "import folium\n",
    "import ast\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pd.read_csv(\"/Users/samuele/Desktop/我/CC/FoliaStream/test_ahc/source.csv\")\n",
    "sink = pd.read_csv(\"/Users/samuele/Desktop/我/CC/FoliaStream/test_ahc/sink.csv\")\n",
    "matrix = pd.read_csv(\"/Users/samuele/Desktop/我/CC/FoliaStream/test_ahc/matrix.csv\").drop('Unnamed: 0.1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_cost_segments={\n",
    "  'pipe':{\n",
    "    (0, 50000): 2.5,\n",
    "    (50000, 100000): 1.5,\n",
    "    (100000, 250000): 0.9,\n",
    "    (250000, 500000): 0.6,\n",
    "    (500000, 1000000): 0.5,\n",
    "    (1000000, 2000000): 0.3,\n",
    "    (2000000, 999999999): 0.2},\n",
    "  'truck_ship':{\n",
    "    (0, 50000): 0.3,\n",
    "    (50000, 100000): 0.3,\n",
    "    (100000, 250000): 0.3,\n",
    "    (250000, 500000): 0.3,\n",
    "    (500000, 1000000): 0.3,\n",
    "    (1000000, 2000000): 0.3,\n",
    "    (2000000, 999999999): 0.3}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def network_optimization_levelized(df_source, df_sink, df_cost_matrix, source_id, sink_id, source_capacity, sink_capacity, emission_cost, transport_method, quantity_cost_segments, capture_cost, stock_cost):\n",
    "\n",
    "    df_source[source_id] = df_source[source_id].astype(int)\n",
    "    df_sink[sink_id] = df_sink[sink_id].astype(int)\n",
    "\n",
    "    df_cost_matrix = df_cost_matrix.rename(columns={\"Unnamed: 0\":sink_id})\n",
    "    \n",
    "    # Network initialization\n",
    "    network = pulp.LpProblem(\"Network_problem\", pulp.LpMinimize)\n",
    "\n",
    "    # Generate nodes\n",
    "    source_list = df_source[source_id].astype(str)\n",
    "    sink_list = list(df_sink[sink_id].astype(str))\n",
    "    sink_list.append(\"Atmosphere\")  \n",
    "\n",
    "    nodes = []\n",
    "    for i in source_list:\n",
    "        nodes.append(i)\n",
    "    for i in sink_list:\n",
    "        nodes.append(i)\n",
    "    \n",
    "    # Generate arcs\n",
    "    arcs = []\n",
    "    arc_capacities = {}\n",
    "\n",
    "    for i in source_list:\n",
    "        for j in sink_list:\n",
    "            if j != \"Atmosphere\":\n",
    "                arcs.append((f\"source_id_{i}\", f\"sink_id_{j}\"))\n",
    "                arc_capacities[(f\"source_id_{i}\", f\"sink_id_{j}\")] = 10000000 # 10Mty capacity of pipes\n",
    "        arcs.append((f\"source_id_{i}\",\"Atmosphere\"))\n",
    "        arc_capacities[(f\"source_id_{i}\",\"Atmosphere\")] = 99999999999999999999999 # infinite value for emitting\n",
    "\n",
    "    \n",
    "    # Flow variables\n",
    "    flow_vars = pulp.LpVariable.dicts(\"Flow\", arcs, 0, None, pulp.LpContinuous)\n",
    "\n",
    "    \n",
    "    # Cost functions\n",
    "    for method, data in quantity_cost_segments.items():\n",
    "        if type(list(quantity_cost_segments[method].keys())[0]) == str:\n",
    "            quantity_cost_segments[method] = {ast.literal_eval(k): v for k, v in quantity_cost_segments[method].items()}\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Set sink_id as index of transport cost matrix\n",
    "    transport_cost = df_cost_matrix.set_index(sink_id)\n",
    "\n",
    "    cost_segments = {}\n",
    "    for i in source_list:\n",
    "        for j in sink_list:\n",
    "\n",
    "            cost_segments[(f\"source_id_{i}\",f\"sink_id_{j}\")] = [\n",
    "                (0,50000,quantity_cost_segments[transport_method][(0, 50000)]*transport_cost.at[j,i]),\n",
    "                (50000,100000,quantity_cost_segments[transport_method][(50000,100000)]*transport_cost.at[j,i]),\n",
    "                (100000,250000,quantity_cost_segments[transport_method][(100000,250000)]*transport_cost.at[j,i]),\n",
    "                (250000,500000,quantity_cost_segments[transport_method][(250000,500000)]*transport_cost.at[j,i]),\n",
    "                (500000,1000000,quantity_cost_segments[transport_method][(500000,1000000)]*transport_cost.at[j,i]),\n",
    "                (1000000,2000000,quantity_cost_segments[transport_method][(1000000,2000000)]*transport_cost.at[j,i]),\n",
    "                (2000000,999999999,quantity_cost_segments[transport_method][(2000000,999999999)]*transport_cost.at[j,i]),\n",
    "            ]\n",
    "        cost_segments[(f\"source_id_{i}\",f\"Atmosphere\")] = [(0,1000000000000, 0)]\n",
    "    \n",
    "\n",
    "    # Segment variables\n",
    "    segment_vars = {}\n",
    "    for arc in arcs:\n",
    "        segment_vars[arc] = []\n",
    "        for i, (start, end, slope) in enumerate(cost_segments[arc]):\n",
    "            var = pulp.LpVariable(f\"Segment_{arc}_{i}\", 0, end - start, pulp.LpContinuous)\n",
    "            segment_vars[arc].append((var, start, end, slope))\n",
    "        \n",
    "    # Objective function\n",
    "    # network += pulp.lpSum([var * slope for arc in arcs for var, start, end, slope in segment_vars[arc]]), \"TotalCost\"\n",
    "    \n",
    "    network += (\n",
    "        pulp.lpSum(flow_vars[arc] * capture_cost for arc in arcs if arc[1] != \"Atmosphere\") +  # Capture\n",
    "        pulp.lpSum(var * slope for arc in arcs for (var, _, _, slope) in segment_vars[arc]) +  # Transport\n",
    "        pulp.lpSum(flow_vars[arc] * stock_cost for arc in arcs if arc[1] != \"Atmosphere\") +  # Capture\n",
    "        pulp.lpSum(flow_vars[arc] * emission_cost for arc in arcs if arc[1] == \"Atmosphere\")    # Emission\n",
    "    ), \"TotalCost\"\n",
    "\n",
    "\n",
    "    # Constraints\n",
    "    for i, rsource in df_source.iterrows():\n",
    "        network += sum(flow_vars[(f\"source_id_{int(rsource[source_id])}\", f\"sink_id_{int(rsink[sink_id])}\")] for j, rsink in df_sink.iterrows() if int(rsink[sink_capacity]) != \"Atmosphere\") + flow_vars[(f\"source_id_{int(rsource[source_id])}\", \"Atmosphere\")] == rsource[source_capacity], f\"source_id_{int(rsource[source_id])}_outflow\"\n",
    "    \n",
    "    for i, rsink in df_sink.iterrows():\n",
    "            network += sum(flow_vars[(f\"source_id_{int(rsource[source_id])}\", f\"sink_id_{int(rsink[sink_id])}\")] for j, rsource in df_source.iterrows()) <= int(rsink[sink_capacity]), f\"sink_id_{int(rsink[sink_id])}_inflow\"\n",
    "\n",
    "    for arc in arcs:\n",
    "        network += flow_vars[arc] <= arc_capacities[arc], f\"Capacity_{arc}\"\n",
    "\n",
    "    for arc in arcs:\n",
    "        network += flow_vars[arc] == pulp.lpSum([var for var, start, end, slope in segment_vars[arc]]), f\"Piecewise_{arc}\"\n",
    "    \n",
    "    # Solution\n",
    "    network.solve()\n",
    "\n",
    "    # Check status\n",
    "    status = pulp.LpStatus[network.status]\n",
    "    print(status)\n",
    "\n",
    "    # if status == \"Infeasible\":\n",
    "    #     print(\"Infeasible model\")\n",
    "    # else:\n",
    "    #     for arc in arcs:\n",
    "    #         print(f\"Flow on arc {arc}: {flow_vars[arc].varValue}\")\n",
    "    #     print(\"Total Cost: \", pulp.value(network.objective))\n",
    "\n",
    "    # Export results\n",
    "    results = []\n",
    "\n",
    "    for arc in arcs:\n",
    "        if flow_vars[arc].varValue > 0:\n",
    "            if arc[1] != \"Atmosphere\":\n",
    "                results.append({\n",
    "                    'source_id':arc[0][10:],\n",
    "                    'sink_id':arc[1][8:],\n",
    "                    'co2_transported':flow_vars[arc].varValue\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    'source_id':arc[0][10:],\n",
    "                    'sink_id':arc[1],\n",
    "                    'co2_transported':flow_vars[arc].varValue\n",
    "                })\n",
    "            \n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_cost = 100\n",
    "capture_cost = 100\n",
    "stock_cost = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/01485c42a7a0450585d102a02fd34c1c-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/01485c42a7a0450585d102a02fd34c1c-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 5196 COLUMNS\n",
      "At line 53014 RHS\n",
      "At line 58206 BOUNDS\n",
      "At line 75796 ENDATA\n",
      "Problem MODEL has 5191 rows, 20130 columns and 27720 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 2113 (-3078) rows, 16703 (-3427) columns and 18815 (-8905) elements\n",
      "0  Obj 0 Primal inf 18908984 (33)\n",
      "33  Obj 1.8908984e+09\n",
      "Optimal - objective value 1.8908984e+09\n",
      "After Postsolve, objective 1.8908984e+09, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 1890898383 - 33 iterations time 0.022, Presolve 0.02\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.06   (Wallclock seconds):       0.07\n",
      "\n",
      "Optimal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>sink_id</th>\n",
       "      <th>co2_transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1897479</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>2794440.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897478</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>1036067.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32438922</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>960096.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1897504</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>883825.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32438914</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>802542.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1897481</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>802405.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1897492</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>784826.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32438910</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>784710.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1897508</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>757999.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32438894</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>720891.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32438907</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>677099.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1897477</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>651487.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1897488</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>590116.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1897506</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>563567.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32438902</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>544877.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1897483</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>487222.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1897500</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>481416.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32438904</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>436841.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1897482</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>425262.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1897507</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>422028.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32438920</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>418148.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32438908</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>413326.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1897485</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>378265.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32438898</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>325451.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1897501</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>302950.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1897495</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>283654.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32438919</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>256576.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1897475</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>206558.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32438905</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>197907.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1897496</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>194509.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1897498</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>134729.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32438911</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>96013.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1897499</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>93161.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id     sink_id  co2_transported\n",
       "0    1897479  Atmosphere      2794440.400\n",
       "1    1897478  Atmosphere      1036067.600\n",
       "2   32438922  Atmosphere       960096.780\n",
       "3    1897504  Atmosphere       883825.440\n",
       "4   32438914  Atmosphere       802542.670\n",
       "5    1897481  Atmosphere       802405.950\n",
       "6    1897492  Atmosphere       784826.070\n",
       "7   32438910  Atmosphere       784710.700\n",
       "8    1897508  Atmosphere       757999.270\n",
       "9   32438894  Atmosphere       720891.750\n",
       "10  32438907  Atmosphere       677099.760\n",
       "11   1897477  Atmosphere       651487.300\n",
       "12   1897488  Atmosphere       590116.890\n",
       "13   1897506  Atmosphere       563567.940\n",
       "14  32438902  Atmosphere       544877.970\n",
       "15   1897483  Atmosphere       487222.610\n",
       "16   1897500  Atmosphere       481416.840\n",
       "17  32438904  Atmosphere       436841.210\n",
       "18   1897482  Atmosphere       425262.760\n",
       "19   1897507  Atmosphere       422028.540\n",
       "20  32438920  Atmosphere       418148.580\n",
       "21  32438908  Atmosphere       413326.810\n",
       "22   1897485  Atmosphere       378265.590\n",
       "23  32438898  Atmosphere       325451.440\n",
       "24   1897501  Atmosphere       302950.950\n",
       "25   1897495  Atmosphere       283654.290\n",
       "26  32438919  Atmosphere       256576.730\n",
       "27   1897475  Atmosphere       206558.590\n",
       "28  32438905  Atmosphere       197907.670\n",
       "29   1897496  Atmosphere       194509.740\n",
       "30   1897498  Atmosphere       134729.990\n",
       "31  32438911  Atmosphere        96013.392\n",
       "32   1897499  Atmosphere        93161.576"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_optimization_levelized(source, \n",
    "                               sink, \n",
    "                               matrix, \n",
    "                               'id', \n",
    "                               'id', \n",
    "                               'emission', \n",
    "                               'sum_mid', \n",
    "                               emission_cost, \n",
    "                               'pipe', \n",
    "                               quantity_cost_segments, \n",
    "                               capture_cost, \n",
    "                               stock_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://router.project-osrm.org/table/v1/driving/'\n",
    "transport_cost = {'pipe': {'less_180': 0.08, 'range_180_500': 0.12, 'range_500_750': 0.15, 'range_750_1500': 0.18, 'more_1500': 0.24}, 'truck_ship': {'less_180': 0.36, 'range_180_500': 0.36, 'range_500_750': 0.36, 'range_750_1500': 0.36, 'more_1500': 0.36}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(data, x_ax, y_ax):\n",
    "\n",
    "    coords = []\n",
    "    for i,row in data.iterrows():\n",
    "        coords.append([row[x_ax], row[y_ax]])\n",
    "    \n",
    "    wcss = []\n",
    "    max_clusters = len(data)\n",
    "    for k in range(1,max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(coords)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "    knee_locator = KneeLocator(range(1, max_clusters + 1), wcss, curve=\"convex\", direction=\"decreasing\")\n",
    "    elbow_point = knee_locator.elbow\n",
    "    \n",
    "    # Minimum one cluster\n",
    "    if elbow_point == None:\n",
    "        elbow_point = 1\n",
    "\n",
    "    return elbow_point\n",
    "# Cluster data using weighted measure\n",
    "def create_clusters(data, x_ax, y_ax, k, weight):\n",
    "\n",
    "    coord_x = np.zeros(len(data))\n",
    "    coord_y = np.zeros(len(data))\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        coord_x[i] = data[x_ax][i]\n",
    "        coord_y[i] = data[y_ax][i]\n",
    "        \n",
    "    X = np.array(list(zip(coord_x,coord_y))).reshape(len(coord_x),2)\n",
    "    scaled_weights = (data[weight]/1000).astype(int)\n",
    "    X_weighted = np.repeat(X, scaled_weights, axis=0)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=k).fit(X_weighted)\n",
    "\n",
    "    centroids = gmm.means_.tolist()\n",
    "    centroids = pd.DataFrame(centroids, columns=[x_ax, y_ax])\n",
    "    centroids['id'] = pd.Series(centroids.index)\n",
    "    labels = gmm.predict(X).tolist()\n",
    "\n",
    "    return centroids, labels\n",
    "\n",
    "\n",
    "# Distance matrix MCFII\n",
    "def distance_matrix_II(url, source, sink, source_id, sink_id, source_lat, sink_lat, source_lon, sink_lon, transport_method):\n",
    "\n",
    "    matrix = pd.DataFrame()\n",
    "\n",
    "    if transport_method == 'pipe':\n",
    "\n",
    "        for i, rsink in sink.iterrows():\n",
    "            for j, rsource in source.iterrows():\n",
    "                matrix.at[i,j] = geodesic((float(rsource[source_lat]),float(rsource[source_lon])), (float(rsink[sink_lat]),float(rsink[sink_lon]))).km\n",
    "\n",
    "\n",
    "    elif transport_method == 'truck_ship':\n",
    "\n",
    "        sink = sink.rename(columns={sink_lat:source_lat, sink_lon:source_lon})\n",
    "        combined_df = pd.concat([source, sink], ignore_index=True)\n",
    "        all_coords  = \";\".join([f\"{lon},{lat}\" for lat, lon in zip(combined_df[source_lat], combined_df[source_lon])])\n",
    "\n",
    "        params = {\n",
    "            \"sources\": \";\".join(map(str, range(len(source)))),\n",
    "            \"destinations\": \";\".join(map(str, range(len(source), len(source)+len(sink)))),\n",
    "            \"annotations\": \"distance\"  # Request both\n",
    "        }\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(url + all_coords, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Truck matrix\n",
    "        matrix = pd.DataFrame(data['distances'])\n",
    "\n",
    "        # Add ship transportation (distance from final point)\n",
    "        for col in range(len(matrix.columns)):\n",
    "            matrix[col] = matrix[col] + data['destinations'][0]['distance']\n",
    "\n",
    "        # Transpose and convert to km\n",
    "        matrix = matrix.T/1000\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Error: Wrong transportation method\")\n",
    "\n",
    "    matrix = matrix.set_index(sink[sink_id])\n",
    "    matrix = matrix.rename(columns=source[source_id])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "# Cost matrix source-centr\n",
    "def cost_matrix_source_centr(matrix, method, transport_cost, capture_cost):\n",
    "\n",
    "    for col in matrix.columns:\n",
    "\n",
    "        for id in matrix[col].index:\n",
    "\n",
    "            if matrix.at[id,col]<180:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['less_180'])\n",
    "\n",
    "            elif matrix.at[id,col] >= 180 and matrix.at[id,col] < 500:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_180_500'])\n",
    "            \n",
    "            elif matrix.at[id,col] >= 500 and matrix.at[id,col] < 750:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_500_750'])\n",
    "\n",
    "            elif matrix.at[id,col] >= 750 and matrix.at[id,col] < 1500:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_750_1500'])\n",
    "\n",
    "            else:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['more_1500'])\n",
    "            \n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cost matrix centr-sink\n",
    "def cost_matrix_centr_sink(matrix, method, transport_cost, cluster_size):\n",
    "\n",
    "    for col in matrix.columns:\n",
    "\n",
    "        for id in matrix[col].index:\n",
    "\n",
    "            if matrix.at[id,col]<180:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['less_180'])/cluster_size[col]\n",
    "\n",
    "            elif matrix.at[id,col] >= 180 and matrix.at[id,col] < 500:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_180_500'])/cluster_size[col]\n",
    "            \n",
    "            elif matrix.at[id,col] >= 500 and matrix.at[id,col] < 750:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_500_750'])/cluster_size[col]\n",
    "\n",
    "            elif matrix.at[id,col] >= 750 and matrix.at[id,col] < 1500:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['range_750_1500'])/cluster_size[col]\n",
    "\n",
    "            else:\n",
    "                matrix.at[id,col] = matrix.at[id,col]*float(transport_cost[method]['more_1500'])/cluster_size[col]\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_optimization_klust_levelized(df_source, df_sink, df_cost_matrix, source_id, sink_id, source_capacity, sink_capacity, url, transport_method, transport_cost, emission_cost, capture_cost, quantity_cost_segments, stock_cost):\n",
    "\n",
    "    # Run optimization for all nodes\n",
    "    mcf_I_results = network_optimization_levelized(df_source, df_sink, df_cost_matrix, source_id, sink_id, source_capacity, sink_capacity, emission_cost, transport_method, quantity_cost_segments, capture_cost, stock_cost)\n",
    "\n",
    "    # Extract nodes failed to connect\n",
    "    unconnected_df = mcf_I_results[mcf_I_results['sink_id'] == \"Atmosphere\"]\n",
    "\n",
    "    # Check if clusterization needed \n",
    "    if len(unconnected_df) == 0:\n",
    "        # No need clustering\n",
    "        return mcf_I_results\n",
    "    else:\n",
    "        # Need clustering\n",
    "        df_source[source_id] = pd.Series(df_source[source_id].astype(str))\n",
    "        df_sink[sink_id] = pd.Series(df_sink[sink_id].astype(str))\n",
    "\n",
    "        df_source = df_source.rename(columns={source_id:f'source_{source_id}'})\n",
    "        df_sink = df_sink.rename(columns={sink_id:f'sink_{sink_id}'})\n",
    "\n",
    "        unconnected_df = pd.merge(unconnected_df, df_source, how='left', on='source_id')   \n",
    "        unconnected_df = unconnected_df.rename(columns={'lat':'source_lat', 'lon':'source_lon'})    \n",
    "\n",
    "        # Cluster the unconnected nodes\n",
    "        el_point = elbow_method(unconnected_df, 'source_lat', 'source_lon')\n",
    "        unconnected_df = unconnected_df.reset_index(drop=True)\n",
    "        centr, labs = create_clusters(unconnected_df, 'source_lat', 'source_lon', el_point, 'co2_transported')\n",
    "\n",
    "        # Merge centroids and unconndected points\n",
    "        unconnected_df['cluster_lab'] = pd.Series(labs)\n",
    "        centr = centr.rename(columns={'id':'cluster_lab','source_lat':'cluster_lat','source_lon':'cluster_lon'})\n",
    "        unconnected_df = pd.merge(unconnected_df, centr, on='cluster_lab', how='left')\n",
    "\n",
    "        # New distance and cost matrix\n",
    "\n",
    "        # Count number of nodes per cluster\n",
    "        cluster_size = unconnected_df.groupby('cluster_lab').size()\n",
    "        # Distance source - centroid\n",
    "        matrix_dist_source_centr = distance_matrix_II(url, unconnected_df, centr, 'source_id', 'cluster_lab', \"source_lat\", \"cluster_lat\", \"source_lon\", \"cluster_lon\", transport_method)\n",
    "        # Cost source - centroid (individual)\n",
    "        matrix_cost_source_centr = cost_matrix_source_centr(matrix_dist_source_centr, transport_method, transport_cost, capture_cost)\n",
    "        # Distance centroid - sink\n",
    "        matrix_dist_centr_sink = distance_matrix_II(url, centr, df_sink, 'cluster_lab', 'sink_id', 'cluster_lat', 'latitude', 'cluster_lon', 'longitude', transport_method)\n",
    "        # Cost centroid - sink (shared)\n",
    "        matrix_cost_centr_sink = cost_matrix_centr_sink(matrix_dist_centr_sink, transport_method, transport_cost, cluster_size)\n",
    "\n",
    "        # General cost matrix setup\n",
    "        matrix = pd.DataFrame(index=df_sink['sink_id'], columns=unconnected_df['source_id'])\n",
    "        for i in matrix.index: #sinks\n",
    "            for j in matrix.columns: #sources\n",
    "                matrix.at[i,j] = matrix_cost_source_centr.loc[unconnected_df.set_index('source_id')['cluster_lab'].loc[j], j]\n",
    "                matrix.at[i,j] = matrix.at[i,j] + matrix_cost_centr_sink.loc[i, unconnected_df.set_index('source_id')['cluster_lab'].loc[j]]\n",
    "        atmosphere_row = pd.DataFrame({col:emission_cost for col in matrix.columns}, index=[\"Atmosphere\"])\n",
    "        matrix = pd.concat([matrix, atmosphere_row], axis=0)\n",
    "        matrix = matrix.reset_index().rename(columns={'index':'Unnamed: 0'})\n",
    "\n",
    "        # Run optimization for clustered nodes ()\n",
    "        mcf_II_results = network_optimization_levelized(unconnected_df, df_sink, matrix, \"source_id\", \"sink_id\", source_capacity, sink_capacity, emission_cost, transport_method, quantity_cost_segments, capture_cost, stock_cost)\n",
    "\n",
    "\n",
    "        # Merge results\n",
    "        mcf_II_results['source_id'] = mcf_II_results['source_id'].astype(int)\n",
    "        mcf_II_results = mcf_II_results.merge(unconnected_df[['source_id', 'cluster_lab', 'cluster_lat', 'cluster_lon']], on='source_id', how='left')\n",
    "\n",
    "        # results = mcf_I_results.merge(mcf_II_results[['source_id','sink_id','cluster_lab','cluster_lat','cluster_lon']],\n",
    "        #                               on='source_id',\n",
    "        #                               how='left',\n",
    "        #                               suffixes=('_I','_II'))\n",
    "        \n",
    "        # results['sink_id'] = results['sink_id_II'].combine_first(results['sink_id_I'])\n",
    "        # cluster_update = ~results['sink_id_II'].isna()\n",
    "        # results['cluster_lab'] = results['cluster_lab'].where(cluster_update, None)\n",
    "        # results['cluster_lat'] = results['cluster_lat'].where(cluster_update, None)\n",
    "        # results['cluster_lon'] = results['cluster_lon'].where(cluster_update, None)\n",
    "\n",
    "        # results = results.drop(columns=['sink_id_I', 'sink_id_II'])\n",
    "\n",
    "\n",
    "        mcf_I_results['source_id'] = mcf_I_results['source_id'].astype(int)\n",
    "        merged = mcf_I_results.merge(mcf_II_results[['source_id', 'sink_id']], \n",
    "                  on='source_id', \n",
    "                  how='left',\n",
    "                  suffixes=('_original', '_updated'))\n",
    "\n",
    "        is_updated = (merged['sink_id_original'] == 'Atmosphere') & (merged['sink_id_updated'] != 'Atmosphere')\n",
    "\n",
    "        merged = merged.merge(mcf_II_results[['source_id', 'cluster_lab', 'cluster_lat', 'cluster_lon']], \n",
    "                     on='source_id', \n",
    "                     how='left')\n",
    "        \n",
    "        results = mcf_I_results.copy()\n",
    "        results['sink_id'] = merged['sink_id_updated'].combine_first(merged['sink_id_original'])\n",
    "        results['cluster_lab'] = merged['cluster_lab'].where(is_updated, np.nan)\n",
    "        results['cluster_lat'] = merged['cluster_lat'].where(is_updated, np.nan)\n",
    "        results['cluster_lon'] = merged['cluster_lon'].where(is_updated, np.nan)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_cost=10\n",
    "emission_cost=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/7def82b77e5741c5ad383f74a2085f46-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/7def82b77e5741c5ad383f74a2085f46-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 5196 COLUMNS\n",
      "At line 53014 RHS\n",
      "At line 58206 BOUNDS\n",
      "At line 75796 ENDATA\n",
      "Problem MODEL has 5191 rows, 20130 columns and 27720 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 0 (-5191) rows, 0 (-20130) columns and 0 (-27720) elements\n",
      "Empty problem - 0 rows, 0 columns and 0 elements\n",
      "Optimal - objective value 8.3467125e+08\n",
      "After Postsolve, objective 8.3467125e+08, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 834671245 - 0 iterations time 0.012, Presolve 0.01\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.04   (Wallclock seconds):       0.05\n",
      "\n",
      "Optimal\n",
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/2c63b077ff2747a6a0845a2db9242f67-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/2c63b077ff2747a6a0845a2db9242f67-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 2561 COLUMNS\n",
      "At line 25746 RHS\n",
      "At line 28303 BOUNDS\n",
      "At line 36832 ENDATA\n",
      "Problem MODEL has 2556 rows, 9760 columns and 13440 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 0 (-2556) rows, 0 (-9760) columns and 0 (-13440) elements\n",
      "Empty problem - 0 rows, 0 columns and 0 elements\n",
      "Optimal - objective value 3.8144107e+08\n",
      "After Postsolve, objective 3.8144107e+08, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 381441066 - 0 iterations time 0.002, Presolve 0.00\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.02   (Wallclock seconds):       0.03\n",
      "\n",
      "Optimal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>sink_id</th>\n",
       "      <th>co2_transported</th>\n",
       "      <th>cluster_lab</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1897479</td>\n",
       "      <td>10402</td>\n",
       "      <td>2794440.400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897478</td>\n",
       "      <td>10397</td>\n",
       "      <td>1036067.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32438922</td>\n",
       "      <td>10402</td>\n",
       "      <td>960096.780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1897504</td>\n",
       "      <td>10386</td>\n",
       "      <td>883825.440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32438914</td>\n",
       "      <td>10397</td>\n",
       "      <td>802542.670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1897481</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>802405.950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1897492</td>\n",
       "      <td>10402</td>\n",
       "      <td>784826.070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32438910</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>784710.700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1897508</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>757999.270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32438894</td>\n",
       "      <td>10392</td>\n",
       "      <td>720891.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32438907</td>\n",
       "      <td>10392</td>\n",
       "      <td>677099.760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1897477</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>651487.300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1897488</td>\n",
       "      <td>10359</td>\n",
       "      <td>590116.890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1897506</td>\n",
       "      <td>10379</td>\n",
       "      <td>563567.940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32438902</td>\n",
       "      <td>10379</td>\n",
       "      <td>544877.970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1897483</td>\n",
       "      <td>10418</td>\n",
       "      <td>487222.610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1897500</td>\n",
       "      <td>10366</td>\n",
       "      <td>481416.840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32438904</td>\n",
       "      <td>10397</td>\n",
       "      <td>436841.210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1897482</td>\n",
       "      <td>10397</td>\n",
       "      <td>425262.760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1897507</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>422028.540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32438920</td>\n",
       "      <td>10397</td>\n",
       "      <td>418148.580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32438908</td>\n",
       "      <td>10379</td>\n",
       "      <td>413326.810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1897485</td>\n",
       "      <td>10393</td>\n",
       "      <td>378265.590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32438898</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>325451.440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1897501</td>\n",
       "      <td>10366</td>\n",
       "      <td>302950.950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1897495</td>\n",
       "      <td>10397</td>\n",
       "      <td>283654.290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32438919</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>256576.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1897475</td>\n",
       "      <td>10403</td>\n",
       "      <td>206558.590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32438905</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>197907.670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1897496</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>194509.740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1897498</td>\n",
       "      <td>10397</td>\n",
       "      <td>134729.990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.926654</td>\n",
       "      <td>-100.307018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32438911</td>\n",
       "      <td>10380</td>\n",
       "      <td>96013.392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1897499</td>\n",
       "      <td>10369</td>\n",
       "      <td>93161.576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_id     sink_id  co2_transported  cluster_lab  cluster_lat  \\\n",
       "0     1897479       10402      2794440.400          NaN          NaN   \n",
       "1     1897478       10397      1036067.600          0.0    19.926654   \n",
       "2    32438922       10402       960096.780          NaN          NaN   \n",
       "3     1897504       10386       883825.440          NaN          NaN   \n",
       "4    32438914       10397       802542.670          0.0    19.926654   \n",
       "5     1897481  Atmosphere       802405.950          NaN          NaN   \n",
       "6     1897492       10402       784826.070          NaN          NaN   \n",
       "7    32438910  Atmosphere       784710.700          NaN          NaN   \n",
       "8     1897508  Atmosphere       757999.270          NaN          NaN   \n",
       "9    32438894       10392       720891.750          NaN          NaN   \n",
       "10   32438907       10392       677099.760          NaN          NaN   \n",
       "11    1897477  Atmosphere       651487.300          NaN          NaN   \n",
       "12    1897488       10359       590116.890          NaN          NaN   \n",
       "13    1897506       10379       563567.940          NaN          NaN   \n",
       "14   32438902       10379       544877.970          NaN          NaN   \n",
       "15    1897483       10418       487222.610          NaN          NaN   \n",
       "16    1897500       10366       481416.840          NaN          NaN   \n",
       "17   32438904       10397       436841.210          0.0    19.926654   \n",
       "18    1897482       10397       425262.760          0.0    19.926654   \n",
       "19    1897507  Atmosphere       422028.540          NaN          NaN   \n",
       "20   32438920       10397       418148.580          0.0    19.926654   \n",
       "21   32438908       10379       413326.810          NaN          NaN   \n",
       "22    1897485       10393       378265.590          NaN          NaN   \n",
       "23   32438898  Atmosphere       325451.440          NaN          NaN   \n",
       "24    1897501       10366       302950.950          NaN          NaN   \n",
       "25    1897495       10397       283654.290          0.0    19.926654   \n",
       "26   32438919  Atmosphere       256576.730          NaN          NaN   \n",
       "27    1897475       10403       206558.590          NaN          NaN   \n",
       "28   32438905  Atmosphere       197907.670          NaN          NaN   \n",
       "29    1897496  Atmosphere       194509.740          NaN          NaN   \n",
       "30    1897498       10397       134729.990          0.0    19.926654   \n",
       "31   32438911       10380        96013.392          NaN          NaN   \n",
       "32    1897499       10369        93161.576          NaN          NaN   \n",
       "\n",
       "    cluster_lon  \n",
       "0           NaN  \n",
       "1   -100.307018  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4   -100.307018  \n",
       "5           NaN  \n",
       "6           NaN  \n",
       "7           NaN  \n",
       "8           NaN  \n",
       "9           NaN  \n",
       "10          NaN  \n",
       "11          NaN  \n",
       "12          NaN  \n",
       "13          NaN  \n",
       "14          NaN  \n",
       "15          NaN  \n",
       "16          NaN  \n",
       "17  -100.307018  \n",
       "18  -100.307018  \n",
       "19          NaN  \n",
       "20  -100.307018  \n",
       "21          NaN  \n",
       "22          NaN  \n",
       "23          NaN  \n",
       "24          NaN  \n",
       "25  -100.307018  \n",
       "26          NaN  \n",
       "27          NaN  \n",
       "28          NaN  \n",
       "29          NaN  \n",
       "30  -100.307018  \n",
       "31          NaN  \n",
       "32          NaN  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_optimization_klust_levelized(source, sink, matrix, 'id', 'id', 'emission', 'sum_mid', url, 'truck_ship', transport_cost, emission_cost, capture_cost, quantity_cost_segments, stock_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results_with_cluster_connections(all_results, cluster_results, unconnected_df):\n",
    "    \"\"\"\n",
    "    Updates the master results DataFrame with new connections from cluster optimization\n",
    "    \n",
    "    Args:\n",
    "        all_results: pd.DataFrame - Master results from all previous iterations\n",
    "        cluster_results: pd.DataFrame - Results from current cluster optimization\n",
    "        unconnected_df: pd.DataFrame - Sources that were unconnected before this iteration\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame - Updated results with new cluster connections\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    updated_results = all_results.copy()\n",
    "    \n",
    "    # Mark which sources were processed in this clustering iteration\n",
    "    processed_sources = unconnected_df['source_id'].unique()\n",
    "    \n",
    "    # First, set previously unconnected sources to 'pending' status\n",
    "    updated_results.loc[\n",
    "        updated_results['source_id'].isin(processed_sources) & \n",
    "        (updated_results['sink_id'] == \"Atmosphere\"),\n",
    "        'sink_id'\n",
    "    ] = \"pending_cluster\"\n",
    "    \n",
    "    # Prepare cluster connection data\n",
    "    cluster_connections = cluster_results[\n",
    "        (cluster_results['sink_id'] != \"Atmosphere\") & \n",
    "        (~cluster_results['sink_id'].str.startswith('cluster_'))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(cluster_connections) > 0:\n",
    "        # These are direct connections from clusters to final sinks\n",
    "        cluster_connections['cluster_level'] = 1\n",
    "        updated_results = pd.concat([updated_results, cluster_connections], ignore_index=True)\n",
    "    \n",
    "    # Handle intra-cluster connections (source to centroid)\n",
    "    intra_cluster = cluster_results[\n",
    "        cluster_results['sink_id'].str.startswith('cluster_')\n",
    "    ].copy()\n",
    "    \n",
    "    if len(intra_cluster) > 0:\n",
    "        # Add cluster metadata\n",
    "        intra_cluster = intra_cluster.merge(\n",
    "            unconnected_df[['source_id', 'cluster_lab', 'cluster_lat', 'cluster_lon']],\n",
    "            on='source_id',\n",
    "            how='left'\n",
    "        )\n",
    "        intra_cluster['cluster_level'] = 1\n",
    "        updated_results = pd.concat([updated_results, intra_cluster], ignore_index=True)\n",
    "    \n",
    "    # Clean up any remaining pending connections\n",
    "    updated_results.loc[\n",
    "        (updated_results['sink_id'] == \"pending_cluster\"),\n",
    "        'sink_id'\n",
    "    ] = \"Atmosphere\"\n",
    "    \n",
    "    # Add hierarchy tracking columns if they don't exist\n",
    "    if 'cluster_hierarchy' not in updated_results.columns:\n",
    "        updated_results['cluster_hierarchy'] = None\n",
    "    \n",
    "    # Update hierarchy information\n",
    "    for _, row in cluster_results.iterrows():\n",
    "        source = row['source_id']\n",
    "        sink = row['sink_id']\n",
    "        \n",
    "        if sink != \"Atmosphere\":\n",
    "            if sink.startswith('cluster_'):\n",
    "                # Connection to cluster centroid\n",
    "                hierarchy = f\"L1:{sink}\"\n",
    "            else:\n",
    "                # Direct connection from cluster to final sink\n",
    "                hierarchy = \"L1:direct\"\n",
    "            \n",
    "            # Update hierarchy info\n",
    "            mask = (updated_results['source_id'] == source) & \\\n",
    "                   (updated_results['sink_id'] == sink)\n",
    "            updated_results.loc[mask, 'cluster_hierarchy'] = hierarchy\n",
    "    \n",
    "    return updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(coords, weights=None):\n",
    "    \"\"\"Calculate optimal number of clusters using elbow method with weighted points\"\"\"\n",
    "    # Ensure coordinates are 2D\n",
    "    coords = np.array(coords)\n",
    "    if coords.ndim == 1:\n",
    "        coords = coords.reshape(-1, 1)\n",
    "    \n",
    "    # Prepare weighted coordinates if weights are provided\n",
    "    if weights is not None:\n",
    "        weights = np.array(weights)\n",
    "        scaled_weights = (weights / weights.min()).astype(int)\n",
    "        coords_weighted = np.repeat(coords, scaled_weights, axis=0)\n",
    "    else:\n",
    "        coords_weighted = coords\n",
    "    \n",
    "    wcss = []\n",
    "    max_clusters = min(10, len(np.unique(coords_weighted, axis=0)))\n",
    "    \n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(coords_weighted)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Find elbow point\n",
    "    if len(wcss) > 1:\n",
    "        knee_locator = KneeLocator(\n",
    "            range(1, max_clusters + 1), \n",
    "            wcss, \n",
    "            curve=\"convex\", \n",
    "            direction=\"decreasing\"\n",
    "        )\n",
    "        optimal_clusters = knee_locator.elbow or 1\n",
    "    else:\n",
    "        optimal_clusters = 1\n",
    "    \n",
    "    return optimal_clusters\n",
    "\n",
    "def create_clusters(coords, k, weights=None):\n",
    "    \"\"\"Create clusters using Gaussian Mixture Model with weights\"\"\"\n",
    "    coords = np.array(coords)\n",
    "    if coords.ndim == 1:\n",
    "        coords = coords.reshape(-1, 1)\n",
    "    \n",
    "    if weights is not None:\n",
    "        weights = np.array(weights)\n",
    "        scaled_weights = (weights / weights.min()).astype(int)\n",
    "        coords_weighted = np.repeat(coords, scaled_weights, axis=0)\n",
    "        gmm = GaussianMixture(n_components=k).fit(coords_weighted)\n",
    "        labels = gmm.predict(coords)  # Predict on original coordinates\n",
    "    else:\n",
    "        gmm = GaussianMixture(n_components=k).fit(coords)\n",
    "        labels = gmm.predict(coords)\n",
    "    \n",
    "    # Calculate centers from weighted points\n",
    "    if weights is not None:\n",
    "        weighted_coords = np.concatenate([coords, weights.reshape(-1, 1)], axis=1)\n",
    "        centers = np.array([\n",
    "            np.average(\n",
    "                weighted_coords[labels == i, :-1], \n",
    "                axis=0, \n",
    "                weights=weighted_coords[labels == i, -1]\n",
    "            ) for i in range(k)\n",
    "        ])\n",
    "    else:\n",
    "        centers = gmm.means_\n",
    "    \n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_optimization_iterative_clustering(df_source, df_sink, df_cost_matrix, source_id, sink_id, \n",
    "                                            source_capacity, sink_capacity, url, transport_method, \n",
    "                                            transport_cost, emission_cost, capture_cost, \n",
    "                                            quantity_cost_segments, stock_cost):\n",
    "    \n",
    "    # First optimization - direct connections\n",
    "    results = network_optimization_levelized(\n",
    "        df_source.copy(),\n",
    "        df_sink.copy(),\n",
    "        df_cost_matrix.copy(),\n",
    "        source_id,\n",
    "        sink_id,\n",
    "        source_capacity,\n",
    "        sink_capacity,\n",
    "        emission_cost,\n",
    "        transport_method,\n",
    "        quantity_cost_segments,\n",
    "        capture_cost,\n",
    "        stock_cost\n",
    "    )\n",
    "    \n",
    "    # Identify unconnected nodes\n",
    "    unconnected_df = results[results['sink_id'] == \"Atmosphere\"].copy()\n",
    "    \n",
    "    iteration = 1\n",
    "    prev_centroids = pd.DataFrame()\n",
    "\n",
    "    while len(unconnected_df) >= 2:\n",
    "        print(f\"\\nStarting clustering iteration {iteration} with {len(unconnected_df)} unconnected nodes\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Prepare source data with proper column handling\n",
    "            df_source_prep = df_source.copy()\n",
    "            df_source_prep[source_id] = df_source_prep[source_id].astype(str)\n",
    "            unconnected_df['source_id'] = unconnected_df['source_id'].astype(str)\n",
    "            \n",
    "            # 2. Safer merge operation\n",
    "            merge_columns = [source_id, 'lat', 'lon']\n",
    "            unconnected_df = unconnected_df.merge(\n",
    "                df_source_prep[merge_columns],\n",
    "                how='left',\n",
    "                left_on='source_id',\n",
    "                right_on=source_id,\n",
    "                suffixes=('', '_src')\n",
    "            )\n",
    "            \n",
    "            # Clean up merged columns\n",
    "            if f'{source_id}_src' in unconnected_df.columns:\n",
    "                unconnected_df = unconnected_df.drop(columns=[f'{source_id}_src'])\n",
    "            \n",
    "            unconnected_df = unconnected_df.rename(columns={\n",
    "                'lat': 'source_lat',\n",
    "                'lon': 'source_lon'\n",
    "            })\n",
    "            \n",
    "            # 3. Cluster with transport-aware weights\n",
    "            coords = unconnected_df[['source_lat', 'source_lon']].values\n",
    "            weights = unconnected_df['co2_transported'].values\n",
    "            \n",
    "            el_point = elbow_method(coords, weights)\n",
    "            centr, labs = create_clusters(coords, el_point, weights)\n",
    "            \n",
    "            # 4. Prepare cluster data with transport attributes\n",
    "            unconnected_df['cluster_lab'] = [str(x) for x in labs]\n",
    "            centr = pd.DataFrame({\n",
    "                'cluster_lab': [str(i) for i in range(len(centr))],\n",
    "                'cluster_lat': centr[:, 0],\n",
    "                'cluster_lon': centr[:, 1],\n",
    "                'total_co2': unconnected_df.groupby('cluster_lab')['co2_transported'].sum()\n",
    "            }).reset_index(drop=True)\n",
    "            \n",
    "            # 5. Calculate source-to-centroid transport costs\n",
    "            matrix_dist_source_centr = distance_matrix_II(\n",
    "                url,\n",
    "                unconnected_df.rename(columns={\n",
    "                    'source_id': 'from_id',\n",
    "                    'source_lat': 'from_lat',\n",
    "                    'source_lon': 'from_lon'\n",
    "                }),\n",
    "                centr.rename(columns={\n",
    "                    'cluster_lab': 'to_id',\n",
    "                    'cluster_lat': 'to_lat',\n",
    "                    'cluster_lon': 'to_lon'\n",
    "                }),\n",
    "                'from_id', 'to_id',\n",
    "                \"from_lat\", \"to_lat\",\n",
    "                \"from_lon\", \"to_lon\",\n",
    "                transport_method\n",
    "            )\n",
    "            \n",
    "            # Apply transport cost rates\n",
    "            matrix_cost_source_centr = cost_matrix_source_centr(\n",
    "                matrix_dist_source_centr,\n",
    "                transport_method,\n",
    "                transport_cost,\n",
    "                capture_cost\n",
    "            )\n",
    "            \n",
    "            # 6. Create virtual sources\n",
    "            cluster_sources = centr.copy()\n",
    "            cluster_sources[source_id] = 'cluster_' + cluster_sources['cluster_lab']\n",
    "            cluster_sources[source_capacity] = cluster_sources['total_co2']\n",
    "            \n",
    "            # 7. Prepare sinks (including previous centroids)\n",
    "            if iteration == 1:\n",
    "                all_sinks = df_sink.copy()\n",
    "            else:\n",
    "                prev_centroids[sink_id] = 'cluster_' + prev_centroids['cluster_lab']\n",
    "                prev_centroids = prev_centroids.rename(columns={\n",
    "                    'cluster_lat': 'latitude',\n",
    "                    'cluster_lon': 'longitude'\n",
    "                })\n",
    "                all_sinks = pd.concat([df_sink, prev_centroids], ignore_index=True)\n",
    "            \n",
    "            all_sinks[sink_id] = all_sinks[sink_id].astype(str)\n",
    "            \n",
    "            # 8. Calculate centroid-to-sink transport costs with shared costs\n",
    "            matrix_dist_centr_sink = distance_matrix_II(\n",
    "                url,\n",
    "                centr.rename(columns={\n",
    "                    'cluster_lab': 'from_id',\n",
    "                    'cluster_lat': 'from_lat',\n",
    "                    'cluster_lon': 'from_lon'\n",
    "                }),\n",
    "                all_sinks.rename(columns={\n",
    "                    sink_id: 'to_id',\n",
    "                    'latitude': 'to_lat',\n",
    "                    'longitude': 'to_lon'\n",
    "                }),\n",
    "                'from_id', 'to_id',\n",
    "                \"from_lat\", \"to_lat\",\n",
    "                \"from_lon\", \"to_lon\",\n",
    "                transport_method\n",
    "            )\n",
    "            \n",
    "            cluster_size = unconnected_df.groupby('cluster_lab').size()\n",
    "            matrix_cost_centr_sink = cost_matrix_centr_sink(\n",
    "                matrix_dist_centr_sink,\n",
    "                transport_method,\n",
    "                transport_cost,\n",
    "                cluster_size\n",
    "            )\n",
    "            \n",
    "            # 9. Build combined cost matrix\n",
    "            combined_matrix = pd.DataFrame(columns=['Unnamed: 0'] + unconnected_df['source_id'].tolist())\n",
    "            combined_matrix['Unnamed: 0'] = all_sinks[sink_id].tolist() + ['Atmosphere']\n",
    "            \n",
    "            for idx, sink_row in combined_matrix.iterrows():\n",
    "                sink_id_val = str(sink_row['Unnamed: 0'])\n",
    "                for source in unconnected_df['source_id']:\n",
    "                    source = str(source)\n",
    "                    cluster_id = unconnected_df.loc[unconnected_df['source_id'] == source, 'cluster_lab'].values[0]\n",
    "                    \n",
    "                    # Source to centroid cost\n",
    "                    source_centr_cost = matrix_cost_source_centr.at[cluster_id, source]\n",
    "                    \n",
    "                    if sink_id_val == 'Atmosphere':\n",
    "                        combined_matrix.at[idx, source] = emission_cost\n",
    "                    else:\n",
    "                        # Centroid to sink cost\n",
    "                        centroid_sink_cost = matrix_cost_centr_sink.at[sink_id_val, cluster_id]\n",
    "                        combined_matrix.at[idx, source] = source_centr_cost + centroid_sink_cost\n",
    "            \n",
    "            # 10. Run optimization\n",
    "            cluster_results = network_optimization_levelized(\n",
    "                unconnected_df.rename(columns={'source_id': source_id}),\n",
    "                all_sinks.rename(columns={sink_id: sink_id}),\n",
    "                combined_matrix,\n",
    "                source_id,\n",
    "                sink_id,\n",
    "                source_capacity,\n",
    "                sink_capacity,\n",
    "                emission_cost,\n",
    "                transport_method,\n",
    "                quantity_cost_segments,\n",
    "                capture_cost,\n",
    "                stock_cost\n",
    "            )\n",
    "            \n",
    "            # Update results\n",
    "            results = update_results_with_cluster_connections(results, cluster_results, unconnected_df)\n",
    "            prev_centroids = centr.copy()\n",
    "            unconnected_df = results[results['sink_id'] == \"Atmosphere\"].copy()\n",
    "            iteration += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {iteration}: {str(e)}\")\n",
    "            print(\"Current columns in unconnected_df:\", unconnected_df.columns.tolist())\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/15d796d784654e5a85be0d0f8b49bb8c-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/15d796d784654e5a85be0d0f8b49bb8c-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 5196 COLUMNS\n",
      "At line 53014 RHS\n",
      "At line 58206 BOUNDS\n",
      "At line 75796 ENDATA\n",
      "Problem MODEL has 5191 rows, 20130 columns and 27720 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 0 (-5191) rows, 0 (-20130) columns and 0 (-27720) elements\n",
      "Empty problem - 0 rows, 0 columns and 0 elements\n",
      "Optimal - objective value 8.3467125e+08\n",
      "After Postsolve, objective 8.3467125e+08, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 834671245 - 0 iterations time 0.012, Presolve 0.01\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.04   (Wallclock seconds):       0.06\n",
      "\n",
      "Optimal\n",
      "\n",
      "Starting clustering iteration 1 with 16 unconnected nodes\n",
      "Error in iteration 1: 'id'\n",
      "Current columns in unconnected_df: ['source_id', 'sink_id', 'co2_transported', 'id', 'source_lat', 'source_lon', 'cluster_lab']\n"
     ]
    }
   ],
   "source": [
    "results = network_optimization_iterative_clustering(source, sink, matrix, 'id', 'id', \n",
    "                                            'emission', 'sum_mid', url, 'truck_ship', \n",
    "                                            transport_cost, emission_cost, capture_cost, \n",
    "                                            quantity_cost_segments, stock_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>sink_id</th>\n",
       "      <th>co2_transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1897479</td>\n",
       "      <td>10402</td>\n",
       "      <td>2794440.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897478</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>1036067.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32438922</td>\n",
       "      <td>10402</td>\n",
       "      <td>960096.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1897504</td>\n",
       "      <td>10386</td>\n",
       "      <td>883825.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32438914</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>802542.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1897481</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>802405.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1897492</td>\n",
       "      <td>10402</td>\n",
       "      <td>784826.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32438910</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>784710.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1897508</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>757999.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32438894</td>\n",
       "      <td>10392</td>\n",
       "      <td>720891.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32438907</td>\n",
       "      <td>10392</td>\n",
       "      <td>677099.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1897477</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>651487.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1897488</td>\n",
       "      <td>10359</td>\n",
       "      <td>590116.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1897506</td>\n",
       "      <td>10379</td>\n",
       "      <td>563567.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32438902</td>\n",
       "      <td>10379</td>\n",
       "      <td>544877.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1897483</td>\n",
       "      <td>10418</td>\n",
       "      <td>487222.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1897500</td>\n",
       "      <td>10366</td>\n",
       "      <td>481416.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32438904</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>436841.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1897482</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>425262.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1897507</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>422028.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32438920</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>418148.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32438908</td>\n",
       "      <td>10379</td>\n",
       "      <td>413326.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1897485</td>\n",
       "      <td>10393</td>\n",
       "      <td>378265.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32438898</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>325451.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1897501</td>\n",
       "      <td>10366</td>\n",
       "      <td>302950.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1897495</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>283654.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32438919</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>256576.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1897475</td>\n",
       "      <td>10403</td>\n",
       "      <td>206558.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32438905</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>197907.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1897496</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>194509.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1897498</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>134729.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32438911</td>\n",
       "      <td>10380</td>\n",
       "      <td>96013.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1897499</td>\n",
       "      <td>10369</td>\n",
       "      <td>93161.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id     sink_id  co2_transported\n",
       "0    1897479       10402      2794440.400\n",
       "1    1897478  Atmosphere      1036067.600\n",
       "2   32438922       10402       960096.780\n",
       "3    1897504       10386       883825.440\n",
       "4   32438914  Atmosphere       802542.670\n",
       "5    1897481  Atmosphere       802405.950\n",
       "6    1897492       10402       784826.070\n",
       "7   32438910  Atmosphere       784710.700\n",
       "8    1897508  Atmosphere       757999.270\n",
       "9   32438894       10392       720891.750\n",
       "10  32438907       10392       677099.760\n",
       "11   1897477  Atmosphere       651487.300\n",
       "12   1897488       10359       590116.890\n",
       "13   1897506       10379       563567.940\n",
       "14  32438902       10379       544877.970\n",
       "15   1897483       10418       487222.610\n",
       "16   1897500       10366       481416.840\n",
       "17  32438904  Atmosphere       436841.210\n",
       "18   1897482  Atmosphere       425262.760\n",
       "19   1897507  Atmosphere       422028.540\n",
       "20  32438920  Atmosphere       418148.580\n",
       "21  32438908       10379       413326.810\n",
       "22   1897485       10393       378265.590\n",
       "23  32438898  Atmosphere       325451.440\n",
       "24   1897501       10366       302950.950\n",
       "25   1897495  Atmosphere       283654.290\n",
       "26  32438919  Atmosphere       256576.730\n",
       "27   1897475       10403       206558.590\n",
       "28  32438905  Atmosphere       197907.670\n",
       "29   1897496  Atmosphere       194509.740\n",
       "30   1897498  Atmosphere       134729.990\n",
       "31  32438911       10380        96013.392\n",
       "32   1897499       10369        93161.576"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_optimization_iterative_clustering(df_source, df_sink, df_cost_matrix, source_id, sink_id, \n",
    "                                            source_capacity, sink_capacity, url, transport_method, \n",
    "                                            transport_cost, emission_cost, capture_cost, \n",
    "                                            quantity_cost_segments, stock_cost):\n",
    "    \n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    df_source = df_source.copy()\n",
    "    df_sink = df_sink.copy()\n",
    "    df_cost_matrix = df_cost_matrix.copy()\n",
    "    \n",
    "    # First optimization - direct connections\n",
    "    results = network_optimization_levelized(\n",
    "        df_source,\n",
    "        df_sink,\n",
    "        df_cost_matrix,\n",
    "        source_id,\n",
    "        sink_id,\n",
    "        source_capacity,\n",
    "        sink_capacity,\n",
    "        emission_cost,\n",
    "        transport_method,\n",
    "        quantity_cost_segments,\n",
    "        capture_cost,\n",
    "        stock_cost\n",
    "    )\n",
    "    \n",
    "    # Identify unconnected nodes\n",
    "    unconnected_df = results[results['sink_id'] == \"Atmosphere\"].copy()\n",
    "    \n",
    "    iteration = 1\n",
    "    prev_centroids = pd.DataFrame()\n",
    "\n",
    "    while len(unconnected_df) >= 2:\n",
    "        print(f\"\\nStarting clustering iteration {iteration} with {len(unconnected_df)} unconnected nodes\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Prepare source data - handle column names carefully\n",
    "            required_source_columns = [source_id, 'lat', 'lon']\n",
    "            if not all(col in df_source.columns for col in required_source_columns):\n",
    "                raise ValueError(f\"Source dataframe missing required columns. Needs: {required_source_columns}\")\n",
    "            \n",
    "            # Clean column names before merge\n",
    "            unconnected_df = unconnected_df.rename(columns={\n",
    "                'source_id': 'tmp_source_id'\n",
    "            })\n",
    "            \n",
    "            # 2. Merge with source data\n",
    "            unconnected_df = pd.merge(\n",
    "                unconnected_df,\n",
    "                df_source[required_source_columns],\n",
    "                how='left',\n",
    "                left_on='tmp_source_id',\n",
    "                right_on=source_id,\n",
    "                suffixes=('', '_src')\n",
    "            ).rename(columns={\n",
    "                'lat': 'source_lat',\n",
    "                'lon': 'source_lon',\n",
    "                'tmp_source_id': 'source_id'\n",
    "            })\n",
    "            \n",
    "            # Clean up merged columns\n",
    "            if f'{source_id}_src' in unconnected_df.columns:\n",
    "                unconnected_df = unconnected_df.drop(columns=[f'{source_id}_src'])\n",
    "            \n",
    "            # 3. Cluster the unconnected nodes with transport-aware weights\n",
    "            coords = unconnected_df[['source_lat', 'source_lon']].values\n",
    "            weights = unconnected_df['co2_transported'].values\n",
    "            \n",
    "            # Calculate optimal clusters considering transport costs\n",
    "            optimal_clusters = calculate_optimal_clusters(coords, weights)\n",
    "            centr, labs = create_weighted_clusters(coords, optimal_clusters, weights)\n",
    "            \n",
    "            # 4. Prepare cluster data with transport attributes\n",
    "            unconnected_df['cluster_lab'] = [str(x) for x in labs]\n",
    "            centr_df = pd.DataFrame({\n",
    "                'cluster_lab': [str(i) for i in range(len(centr))],\n",
    "                'cluster_lat': centr[:, 0],\n",
    "                'cluster_lon': centr[:, 1],\n",
    "                'total_co2': unconnected_df.groupby('cluster_lab')['co2_transported'].sum()\n",
    "            }).reset_index(drop=True)\n",
    "            \n",
    "            # 5. Calculate source-to-centroid transport costs\n",
    "            print(\"Calculating source-to-centroid transport costs...\")\n",
    "            matrix_dist_source_centr = distance_matrix_II(\n",
    "                url,\n",
    "                unconnected_df.rename(columns={\n",
    "                    'source_id': 'from_id',\n",
    "                    'source_lat': 'from_lat',\n",
    "                    'source_lon': 'from_lon'\n",
    "                }),\n",
    "                centr_df.rename(columns={\n",
    "                    'cluster_lab': 'to_id',\n",
    "                    'cluster_lat': 'to_lat',\n",
    "                    'cluster_lon': 'to_lon'\n",
    "                }),\n",
    "                'from_id', 'to_id',\n",
    "                \"from_lat\", \"to_lat\",\n",
    "                \"from_lon\", \"to_lon\",\n",
    "                transport_method\n",
    "            )\n",
    "            \n",
    "            # Apply transport cost rates\n",
    "            matrix_cost_source_centr = cost_matrix_source_centr(\n",
    "                matrix_dist_source_centr,\n",
    "                transport_method,\n",
    "                transport_cost,\n",
    "                capture_cost\n",
    "            )\n",
    "            \n",
    "            # [Rest of the implementation remains the same...]\n",
    "            \n",
    "            # Update results and prepare for next iteration\n",
    "            results = update_results_with_cluster_connections(results, cluster_results, unconnected_df)\n",
    "            prev_centroids = centr_df.copy()\n",
    "            unconnected_df = results[results['sink_id'] == \"Atmosphere\"].copy()\n",
    "            iteration += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {iteration}: {str(e)}\")\n",
    "            print(\"Current columns in unconnected_df:\", unconnected_df.columns.tolist())\n",
    "            print(\"Current centroids:\", centr_df.columns if 'centr_df' in locals() else \"Not created\")\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_optimal_clusters(coords, weights=None, max_clusters=10):\n",
    "    \"\"\"Robust elbow method with weighted points\"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    from kneed import KneeLocator\n",
    "    import numpy as np\n",
    "    \n",
    "    coords = np.array(coords)\n",
    "    if coords.ndim == 1:\n",
    "        coords = coords.reshape(-1, 1)\n",
    "    \n",
    "    # Prepare weighted dataset if weights are provided\n",
    "    if weights is not None:\n",
    "        weights = np.array(weights)\n",
    "        scaled_weights = (weights / weights.min()).astype(int)\n",
    "        coords_weighted = np.repeat(coords, scaled_weights, axis=0)\n",
    "    else:\n",
    "        coords_weighted = coords\n",
    "    \n",
    "    max_clusters = min(max_clusters, len(np.unique(coords_weighted, axis=0)))\n",
    "    if max_clusters <= 1:\n",
    "        return 1\n",
    "    \n",
    "    wcss = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(coords_weighted)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    if len(wcss) > 1:\n",
    "        knee_locator = KneeLocator(\n",
    "            range(1, max_clusters + 1),\n",
    "            wcss,\n",
    "            curve='convex',\n",
    "            direction='decreasing'\n",
    "        )\n",
    "        return knee_locator.elbow or 1\n",
    "    return 1\n",
    "\n",
    "\n",
    "def create_weighted_clusters(coords, n_clusters, weights=None):\n",
    "    \"\"\"Create clusters using GMM with proper weight handling\"\"\"\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    import numpy as np\n",
    "    \n",
    "    coords = np.array(coords)\n",
    "    if coords.ndim == 1:\n",
    "        coords = coords.reshape(-1, 1)\n",
    "    \n",
    "    if weights is not None:\n",
    "        weights = np.array(weights)\n",
    "        scaled_weights = (weights / weights.min()).astype(int)\n",
    "        coords_weighted = np.repeat(coords, scaled_weights, axis=0)\n",
    "        gmm = GaussianMixture(n_components=n_clusters).fit(coords_weighted)\n",
    "    else:\n",
    "        gmm = GaussianMixture(n_components=n_clusters).fit(coords)\n",
    "    \n",
    "    labels = gmm.predict(coords)\n",
    "    centers = gmm.means_\n",
    "    \n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/210a3a684c794eacb6d3b5e4994f8529-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /var/folders/2d/n_vsfm_n1_l_wtnsx7bt2g300000gn/T/210a3a684c794eacb6d3b5e4994f8529-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 5196 COLUMNS\n",
      "At line 53014 RHS\n",
      "At line 58206 BOUNDS\n",
      "At line 75796 ENDATA\n",
      "Problem MODEL has 5191 rows, 20130 columns and 27720 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 0 (-5191) rows, 0 (-20130) columns and 0 (-27720) elements\n",
      "Empty problem - 0 rows, 0 columns and 0 elements\n",
      "Optimal - objective value 8.3467125e+08\n",
      "After Postsolve, objective 8.3467125e+08, infeasibilities - dual 0 (0), primal 0 (0)\n",
      "Optimal objective 834671245 - 0 iterations time 0.012, Presolve 0.01\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.04   (Wallclock seconds):       0.06\n",
      "\n",
      "Optimal\n",
      "\n",
      "Starting clustering iteration 1 with 16 unconnected nodes\n",
      "Error in iteration 1: You are trying to merge on object and int64 columns for key 'tmp_source_id'. If you wish to proceed you should use pd.concat\n",
      "Current columns in unconnected_df: ['tmp_source_id', 'sink_id', 'co2_transported']\n",
      "Current centroids: Not created\n"
     ]
    }
   ],
   "source": [
    "results = network_optimization_iterative_clustering(source, sink, matrix, 'id', 'id', \n",
    "                                            'emission', 'sum_mid', url, 'truck_ship', \n",
    "                                            transport_cost, emission_cost, capture_cost, \n",
    "                                            quantity_cost_segments, stock_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def haversine(coord1, coord2):\n",
    "    \"\"\"Calculate Haversine distance between two coordinates\"\"\"\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "    R = 6371  # Earth radius in km\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = (sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2)\n",
    "    return R * 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "def network_optimization_levelized(df_source, df_sink, df_cost_matrix, source_id, sink_id,\n",
    "                                 source_capacity, sink_capacity, emission_cost, transport_method,\n",
    "                                 quantity_cost_segments, capture_cost, stock_cost):\n",
    "    \"\"\"Core network optimization function\"\"\"\n",
    "    try:\n",
    "        # Convert IDs to strings\n",
    "        df_source[source_id] = df_source[source_id].astype(str)\n",
    "        df_sink[sink_id] = df_sink[sink_id].astype(str)\n",
    "        \n",
    "        # Prepare cost matrix\n",
    "        if 'Unnamed: 0' in df_cost_matrix.columns:\n",
    "            df_cost_matrix = df_cost_matrix.rename(columns={'Unnamed: 0': sink_id})\n",
    "        transport_cost = df_cost_matrix.set_index(sink_id)\n",
    "\n",
    "        # Initialize problem\n",
    "        network = pulp.LpProblem(\"Network_Optimization\", pulp.LpMinimize)\n",
    "        \n",
    "        # Nodes and arcs\n",
    "        sources = df_source[source_id].unique()\n",
    "        sinks = df_sink[sink_id].unique().tolist()\n",
    "        \n",
    "        # Add atmosphere as special sink\n",
    "        sinks.append('Atmosphere')\n",
    "        \n",
    "        arcs = []\n",
    "        arc_capacities = {}\n",
    "        for src in sources:\n",
    "            for snk in sinks:\n",
    "                arc = (f\"src_{src}\", f\"snk_{snk}\")\n",
    "                arcs.append(arc)\n",
    "                # Use large but finite capacity\n",
    "                arc_capacities[arc] = 1e12 if snk == 'Atmosphere' else 10_000_000\n",
    "\n",
    "        # Variables\n",
    "        flow_vars = pulp.LpVariable.dicts(\"Flow\", arcs, 0, None, pulp.LpContinuous)\n",
    "        \n",
    "        # Cost segments\n",
    "        cost_segments = {}\n",
    "        for src in sources:\n",
    "            for snk in sinks:\n",
    "                if snk == 'Atmosphere':\n",
    "                    cost_segments[(f\"src_{src}\", f\"snk_{snk}\")] = [(0, 1e12, emission_cost)]\n",
    "                else:\n",
    "                    try:\n",
    "                        cost = transport_cost.at[snk, src]\n",
    "                        cost_segments[(f\"src_{src}\", f\"snk_{snk}\")] = [\n",
    "                            (0, 50000, quantity_cost_segments[transport_method][(0, 50000)] * cost),\n",
    "                            (50000, 100000, quantity_cost_segments[transport_method][(50000, 100000)] * cost),\n",
    "                            (100000, 250000, quantity_cost_segments[transport_method][(100000, 250000)] * cost),\n",
    "                            (250000, 500000, quantity_cost_segments[transport_method][(250000, 500000)] * cost),\n",
    "                            (500000, 1000000, quantity_cost_segments[transport_method][(500000, 1000000)] * cost),\n",
    "                            (1000000, 2000000, quantity_cost_segments[transport_method][(1000000, 2000000)] * cost),\n",
    "                            (2000000, 1e12, quantity_cost_segments[transport_method][(2000000, 999999999)] * cost)\n",
    "                        ]\n",
    "                    except KeyError:\n",
    "                        print(f\"Missing cost for source {src} to sink {snk}\")\n",
    "                        continue\n",
    "\n",
    "        # Segment variables\n",
    "        segment_vars = {}\n",
    "        for arc in arcs:\n",
    "            if arc not in cost_segments:\n",
    "                continue\n",
    "            segment_vars[arc] = []\n",
    "            for i, (start, end, slope) in enumerate(cost_segments[arc]):\n",
    "                upper_bound = end - start\n",
    "                var = pulp.LpVariable(f\"Seg_{arc[0]}_{arc[1]}_{i}\", 0, upper_bound, pulp.LpContinuous)\n",
    "                segment_vars[arc].append((var, slope))\n",
    "\n",
    "        # Objective function\n",
    "        network += (\n",
    "            pulp.lpSum(flow_vars[arc] * capture_cost for arc in arcs if arc[1] != 'snk_Atmosphere') +\n",
    "            pulp.lpSum(var * slope for arc in arcs for (var, slope) in segment_vars.get(arc, [])) +\n",
    "            pulp.lpSum(flow_vars[arc] * stock_cost for arc in arcs if arc[1] != 'snk_Atmosphere') +\n",
    "            pulp.lpSum(flow_vars[arc] * emission_cost for arc in arcs if arc[1] == 'snk_Atmosphere')\n",
    "        ), \"Total_Cost\"\n",
    "\n",
    "        # Constraints\n",
    "        for _, src_row in df_source.iterrows():\n",
    "            src = str(src_row[source_id])\n",
    "            network += (\n",
    "                pulp.lpSum(flow_vars.get((f\"src_{src}\", f\"snk_{snk}\"), 0)\n",
    "                for snk in sinks\n",
    "            ) == src_row[source_capacity], f\"Source_{src}_outflow\")\n",
    "\n",
    "        for _, snk_row in df_sink.iterrows():\n",
    "            snk = str(snk_row[sink_id])\n",
    "            network += (\n",
    "                pulp.lpSum(flow_vars.get((f\"src_{src}\", f\"snk_{snk}\"), 0)\n",
    "                for src in sources\n",
    "            ) <= snk_row[sink_capacity], f\"Sink_{snk}_inflow\")\n",
    "\n",
    "        for arc in arcs:\n",
    "            if arc in segment_vars:\n",
    "                network += flow_vars[arc] == pulp.lpSum(var for (var, _) in segment_vars[arc])\n",
    "                network += flow_vars[arc] <= arc_capacities[arc]\n",
    "\n",
    "        # Solve\n",
    "        network.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for arc in arcs:\n",
    "            if arc in flow_vars and flow_vars[arc].varValue is not None and flow_vars[arc].varValue > 1e-6:\n",
    "                src = arc[0].replace(\"src_\", \"\")\n",
    "                snk = arc[1].replace(\"snk_\", \"\")\n",
    "                results.append({\n",
    "                    'source_id': src,\n",
    "                    'sink_id': snk,\n",
    "                    'co2_transported': flow_vars[arc].varValue,\n",
    "                    'path': f\"{src}→{snk}\"\n",
    "                })\n",
    "        \n",
    "        # Return empty DataFrame with correct columns if no results\n",
    "        if not results:\n",
    "            return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {str(e)}\")\n",
    "        return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n",
    "\n",
    "def iterative_clustered_network_optimization(\n",
    "    df_source, df_sink, df_cost_matrix, source_id, sink_id,\n",
    "    source_capacity, sink_capacity, emission_cost, transport_method,\n",
    "    quantity_cost_segments, capture_cost, stock_cost, max_iter=5\n",
    "):\n",
    "    \"\"\"Main iterative optimization with clustering\"\"\"\n",
    "    try:\n",
    "        # Initialize\n",
    "        df_source = df_source.copy()\n",
    "        df_sink = df_sink.copy()\n",
    "        df_cost_matrix = df_cost_matrix.copy()\n",
    "        \n",
    "        all_results = []\n",
    "        cluster_history = {}\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # Run optimization\n",
    "            current_results = network_optimization_levelized(\n",
    "                df_source, df_sink, df_cost_matrix, source_id, sink_id,\n",
    "                source_capacity, sink_capacity, emission_cost, transport_method,\n",
    "                quantity_cost_segments, capture_cost, stock_cost\n",
    "            )\n",
    "            \n",
    "            # Track connected sources\n",
    "            connected = set(current_results['source_id'])\n",
    "            unconnected = set(df_source[source_id].astype(str)) - connected\n",
    "            \n",
    "            if not unconnected:\n",
    "                break\n",
    "                \n",
    "            # Cluster unconnected sources\n",
    "            X = df_source[df_source[source_id].astype(str).isin(unconnected)]\n",
    "            if len(X) < 2:\n",
    "                break\n",
    "                \n",
    "            # Weighted clustering\n",
    "            k = min(3, len(X))\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            weights = X[source_capacity] / X[source_capacity].sum()\n",
    "            kmeans.fit(X[['lat', 'lon']], sample_weight=weights)\n",
    "            \n",
    "            # Create centroids\n",
    "            centroids = X.groupby(kmeans.labels_).agg({\n",
    "                'lat': 'mean',\n",
    "                'lon': 'mean',\n",
    "                source_id: lambda x: f\"centroid_{iteration}_\" + \"_\".join(sorted(x.astype(str)))\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Update network\n",
    "            for _, centroid in centroids.iterrows():\n",
    "                centroid_id = centroid[source_id]\n",
    "                if centroid_id in cluster_history:\n",
    "                    continue\n",
    "                    \n",
    "                cluster_history[centroid_id] = (centroid['lat'], centroid['lon'])\n",
    "                \n",
    "                # Add centroid to sinks\n",
    "                df_sink = pd.concat([\n",
    "                    df_sink,\n",
    "                    pd.DataFrame([{\n",
    "                        sink_id: centroid_id,\n",
    "                        'latitude': centroid['lat'],\n",
    "                        'longitude': centroid['lon'],\n",
    "                        'sum_mid': float('inf'),\n",
    "                        'site_name': f\"Cluster_{iteration}\"\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "                \n",
    "                # Update cost matrix\n",
    "                cluster_members = X[kmeans.labels_ == _][source_id].astype(str)\n",
    "                for src in cluster_members:\n",
    "                    # Individual connection to centroid\n",
    "                    distance = haversine(\n",
    "                        (df_source[df_source[source_id] == src]['lat'].values[0],\n",
    "                        df_source[df_source[source_id] == src]['lon'].values[0]),\n",
    "                        (centroid['lat'], centroid['lon'])\n",
    "                    )\n",
    "                    df_cost_matrix.loc[df_cost_matrix[sink_id] == centroid_id, src] = distance\n",
    "                    \n",
    "                    # Shared connection from centroid to sinks\n",
    "                    for _, sink_row in df_sink[df_sink[sink_id] != centroid_id].iterrows():\n",
    "                        original_cost = df_cost_matrix.loc[\n",
    "                            df_cost_matrix[sink_id] == sink_row[sink_id], src].values[0]\n",
    "                        shared_cost = original_cost / len(cluster_members)\n",
    "                        df_cost_matrix.loc[\n",
    "                            df_cost_matrix[sink_id] == sink_row[sink_id], centroid_id] = shared_cost\n",
    "\n",
    "            # Store results with path info\n",
    "            for _, row in current_results.iterrows():\n",
    "                if row['sink_id'].startswith('centroid_'):\n",
    "                    centroid = row['sink_id']\n",
    "                    downstream = current_results[current_results['source_id'] == centroid]\n",
    "                    for _, ds_row in downstream.iterrows():\n",
    "                        all_results.append({\n",
    "                            'source_id': row['source_id'],\n",
    "                            'sink_id': ds_row['sink_id'],\n",
    "                            'co2_transported': row['co2_transported'],\n",
    "                            'path': f\"{row['source_id']}→{centroid}→{ds_row['sink_id']}\"\n",
    "                        })\n",
    "                else:\n",
    "                    all_results.append(row.to_dict())\n",
    "\n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Iterative optimization failed: {str(e)}\")\n",
    "        return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example Data Preparation\n",
    "def prepare_example_data():\n",
    "    # Example Sources (Cement Plants)\n",
    "    source_data = {\n",
    "        'id': ['1897479', '1897478', '32438922', '1897504', '32438914'],\n",
    "        'name': ['CEMEX Mexico SA de CV Puebla 1', \n",
    "                'CEMEX Mexico SA de CV Hidalgo 1',\n",
    "                'Holcim Mexico SA de CV Veracruz 1',\n",
    "                'Holcim Mexico SA de CV Tabasco 1',\n",
    "                'Cementos Fortaleza Hidalgo 2'],\n",
    "        'emission': [2.794440e+06, 1.036068e+06, 9.600968e+05, 8.838254e+05, 8.025427e+05],\n",
    "        'lat': [18.967968, 19.996589, 18.857600, 17.651499, 20.011934],\n",
    "        'lon': [-97.961858, -99.210844, -97.041208, -92.443296, -99.178466]\n",
    "    }\n",
    "    df_source = pd.DataFrame(source_data)\n",
    "\n",
    "    # Example Sinks (Storage Sites)\n",
    "    sink_data = {\n",
    "        'id': ['10346', '10347', '10348', '10349', '10350'],\n",
    "        'sum_mid': [4.300000e+08, 1.040000e+09, 1.920000e+09, 1.970000e+09, 5.400000e+08],\n",
    "        'latitude': [25.642778, 25.664722, 25.758611, 25.779444, 25.809444],\n",
    "        'longitude': [-99.071389, -98.954444, -98.580000, -98.438611, -97.863611],\n",
    "        'site_name': ['Burgos B1A', 'Burgos B1B', 'Burgos B1C', 'Burgos B1D', 'Burgos B1E']\n",
    "    }\n",
    "    df_sink = pd.DataFrame(sink_data)\n",
    "\n",
    "    # Example Cost Matrix (Distances between sources and sinks)\n",
    "    cost_matrix_data = {\n",
    "        'Unnamed: 0': ['10346', '10347', '10348', '10349', '10350'],\n",
    "        '1897479': [345.817296, 344.745828, 348.487164, 340.211268, 336.485808],\n",
    "        '1897478': [332.455428, 329.412024, 333.153360, 324.877428, 321.152004],\n",
    "        '32438922': [373.947948, 372.876480, 376.617816, 368.341884, 364.616460],\n",
    "        '1897504': [521.441244, 520.369740, 524.111148, 515.835180, 512.109720],\n",
    "        '32438914': [331.633836, 329.152284, 332.893620, 324.617688, 320.892264]\n",
    "    }\n",
    "    df_cost_matrix = pd.DataFrame(cost_matrix_data)\n",
    "\n",
    "    return df_source, df_sink, df_cost_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['source_id', 'sink_id', 'co2_transported', 'path'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 21\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Run optimization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[39m=\u001b[39m iterative_clustered_network_optimization(\n\u001b[1;32m      6\u001b[0m     df_source\u001b[39m=\u001b[39mdf_source,\n\u001b[1;32m      7\u001b[0m     df_sink\u001b[39m=\u001b[39mdf_sink,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     max_iter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[39mprint\u001b[39m(results[[\u001b[39m'\u001b[39;49m\u001b[39msource_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msink_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mco2_transported\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[39mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[39mif\u001b[39;00m nmissing \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['source_id', 'sink_id', 'co2_transported', 'path'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Prepare data (with string IDs)\n",
    "df_source, df_sink, df_cost_matrix = prepare_example_data()\n",
    "\n",
    "# Run optimization\n",
    "results = iterative_clustered_network_optimization(\n",
    "    df_source=df_source,\n",
    "    df_sink=df_sink,\n",
    "    df_cost_matrix=df_cost_matrix,\n",
    "    source_id='id',\n",
    "    sink_id='id',\n",
    "    source_capacity='emission',\n",
    "    sink_capacity='sum_mid',\n",
    "    emission_cost=50,\n",
    "    transport_method='pipe',\n",
    "    quantity_cost_segments=quantity_cost_segments,\n",
    "    capture_cost=30,\n",
    "    stock_cost=10,\n",
    "    max_iter=5\n",
    ")\n",
    "\n",
    "print(results[['source_id', 'sink_id', 'co2_transported', 'path']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def haversine(coord1, coord2):\n",
    "    \"\"\"Calculate Haversine distance between two lat/lon coordinates.\"\"\"\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
    "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pulp\n",
    "import pandas as pd\n",
    "\n",
    "def network_optimization_levelized(df_source, df_sink, df_cost_matrix, source_id, sink_id,\n",
    "                                    source_capacity, sink_capacity, emission_cost, transport_method,\n",
    "                                    quantity_cost_segments, capture_cost, stock_cost):\n",
    "    \"\"\"Solve the network optimization with levelized cost segmentation.\"\"\"\n",
    "    try:\n",
    "        df_source[source_id] = df_source[source_id].astype(str)\n",
    "        df_sink[sink_id] = df_sink[sink_id].astype(str)\n",
    "\n",
    "        if 'Unnamed: 0' in df_cost_matrix.columns:\n",
    "            df_cost_matrix = df_cost_matrix.rename(columns={'Unnamed: 0': sink_id})\n",
    "        transport_cost = df_cost_matrix.set_index(sink_id)\n",
    "\n",
    "        network = pulp.LpProblem(\"Network_Optimization\", pulp.LpMinimize)\n",
    "\n",
    "        sources = df_source[source_id].unique()\n",
    "        sinks = df_sink[sink_id].unique().tolist()\n",
    "        sinks.append('Atmosphere')  # Add virtual sink\n",
    "\n",
    "        arcs = [(f\"src_{s}\", f\"snk_{t}\") for s in sources for t in sinks]\n",
    "        arc_capacities = {(a, b): 1e12 if 'Atmosphere' in b else 10_000_000 for a, b in arcs}\n",
    "\n",
    "        flow_vars = pulp.LpVariable.dicts(\"Flow\", arcs, 0, None, pulp.LpContinuous)\n",
    "\n",
    "        cost_segments = {}\n",
    "        for s in sources:\n",
    "            for t in sinks:\n",
    "                arc = (f\"src_{s}\", f\"snk_{t}\")\n",
    "                if 'Atmosphere' in t:\n",
    "                    cost_segments[arc] = [(0, 1e12, emission_cost)]\n",
    "                    continue\n",
    "                try:\n",
    "                    base_cost = transport_cost.at[t, s]\n",
    "                    cost_segments[arc] = [\n",
    "                        (start, end, quantity_cost_segments[transport_method][(start, end)] * base_cost)\n",
    "                        for (start, end) in quantity_cost_segments[transport_method]\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "        segment_vars = {}\n",
    "        for arc, segments in cost_segments.items():\n",
    "            segment_vars[arc] = []\n",
    "            for i, (start, end, slope) in enumerate(segments):\n",
    "                var = pulp.LpVariable(f\"Seg_{arc[0]}_{arc[1]}_{i}\", 0, end - start, pulp.LpContinuous)\n",
    "                segment_vars[arc].append((var, slope))\n",
    "\n",
    "        network += (\n",
    "            pulp.lpSum(flow_vars[arc] * capture_cost for arc in arcs if 'Atmosphere' not in arc[1]) +\n",
    "            pulp.lpSum(var * slope for arc in arcs for var, slope in segment_vars.get(arc, [])) +\n",
    "            pulp.lpSum(flow_vars[arc] * stock_cost for arc in arcs if 'Atmosphere' not in arc[1]) +\n",
    "            pulp.lpSum(flow_vars[arc] * emission_cost for arc in arcs if 'Atmosphere' in arc[1])\n",
    "        ), \"Total_Cost\"\n",
    "\n",
    "        for _, row in df_source.iterrows():\n",
    "            src = str(row[source_id])\n",
    "            network += pulp.lpSum(flow_vars.get((f\"src_{src}\", f\"snk_{snk}\"), 0) for snk in sinks) == row[source_capacity]\n",
    "\n",
    "        for _, row in df_sink.iterrows():\n",
    "            snk = str(row[sink_id])\n",
    "            network += pulp.lpSum(flow_vars.get((f\"src_{src}\", f\"snk_{snk}\"), 0) for src in sources) <= row[sink_capacity]\n",
    "\n",
    "        for arc in arcs:\n",
    "            if arc in segment_vars:\n",
    "                network += flow_vars[arc] == pulp.lpSum(var for var, _ in segment_vars[arc])\n",
    "            network += flow_vars[arc] <= arc_capacities[arc]\n",
    "\n",
    "        network.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "        results = []\n",
    "        for arc in arcs:\n",
    "            value = flow_vars[arc].varValue\n",
    "            if value and value > 1e-6:\n",
    "                results.append({\n",
    "                    'source_id': arc[0].replace(\"src_\", \"\"),\n",
    "                    'sink_id': arc[1].replace(\"snk_\", \"\"),\n",
    "                    'co2_transported': value,\n",
    "                    'path': f\"{arc[0].replace('src_', '')}→{arc[1].replace('snk_', '')}\"\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(results, columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed: {e}\")\n",
    "        return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def iterative_clustered_network_optimization(df_source, df_sink, df_cost_matrix,\n",
    "                                             source_id, sink_id, source_capacity, sink_capacity,\n",
    "                                             emission_cost, transport_method, quantity_cost_segments,\n",
    "                                             capture_cost, stock_cost, max_iter=5):\n",
    "    \"\"\"Run iterative optimization with clustering of unconnected sources.\"\"\"\n",
    "    try:\n",
    "        df_source = df_source.copy()\n",
    "        df_sink = df_sink.copy()\n",
    "        df_cost_matrix = df_cost_matrix.copy()\n",
    "\n",
    "        all_results = []\n",
    "        cluster_history = {}\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            current_results = network_optimization_levelized(\n",
    "                df_source, df_sink, df_cost_matrix, source_id, sink_id,\n",
    "                source_capacity, sink_capacity, emission_cost, transport_method,\n",
    "                quantity_cost_segments, capture_cost, stock_cost\n",
    "            )\n",
    "\n",
    "            connected = set(current_results['source_id'])\n",
    "            unconnected = set(df_source[source_id].astype(str)) - connected\n",
    "\n",
    "            if not unconnected or len(unconnected) < 2:\n",
    "                break\n",
    "\n",
    "            X = df_source[df_source[source_id].astype(str).isin(unconnected)]\n",
    "            k = min(3, len(X))\n",
    "            weights = X[source_capacity] / X[source_capacity].sum()\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(X[['lat', 'lon']], sample_weight=weights)\n",
    "\n",
    "            centroids = X.copy()\n",
    "            centroids['cluster'] = kmeans.labels_\n",
    "            for cluster_id, group in centroids.groupby('cluster'):\n",
    "                members = group[source_id].astype(str).tolist()\n",
    "                centroid_lat = group['lat'].mean()\n",
    "                centroid_lon = group['lon'].mean()\n",
    "                centroid_id = f\"centroid_{iteration}_{'_'.join(sorted(members))}\"\n",
    "\n",
    "                if centroid_id in cluster_history:\n",
    "                    continue\n",
    "                cluster_history[centroid_id] = (centroid_lat, centroid_lon)\n",
    "\n",
    "                df_sink = pd.concat([\n",
    "                    df_sink,\n",
    "                    pd.DataFrame([{\n",
    "                        sink_id: centroid_id,\n",
    "                        'latitude': centroid_lat,\n",
    "                        'longitude': centroid_lon,\n",
    "                        'sum_mid': float('inf'),\n",
    "                        'site_name': f\"Cluster_{iteration}\"\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "                for src in members:\n",
    "                    distance = haversine(\n",
    "                        (df_source[df_source[source_id] == src]['lat'].values[0],\n",
    "                         df_source[df_source[source_id] == src]['lon'].values[0]),\n",
    "                        (centroid_lat, centroid_lon)\n",
    "                    )\n",
    "                    df_cost_matrix.loc[df_cost_matrix[sink_id] == centroid_id, src] = distance\n",
    "\n",
    "                    for _, sink_row in df_sink[df_sink[sink_id] != centroid_id].iterrows():\n",
    "                        original_cost = df_cost_matrix.loc[df_cost_matrix[sink_id] == sink_row[sink_id], src].values[0]\n",
    "                        df_cost_matrix.loc[df_cost_matrix[sink_id] == sink_row[sink_id], centroid_id] = original_cost / len(members)\n",
    "\n",
    "            # Post-process results\n",
    "            for _, row in current_results.iterrows():\n",
    "                if row['sink_id'].startswith('centroid_'):\n",
    "                    centroid = row['sink_id']\n",
    "                    downstream = current_results[current_results['source_id'] == centroid]\n",
    "                    for _, ds_row in downstream.iterrows():\n",
    "                        all_results.append({\n",
    "                            'source_id': row['source_id'],\n",
    "                            'sink_id': ds_row['sink_id'],\n",
    "                            'co2_transported': row['co2_transported'],\n",
    "                            'path': f\"{row['source_id']}→{centroid}→{ds_row['sink_id']}\"\n",
    "                        })\n",
    "                else:\n",
    "                    all_results.append(row.to_dict())\n",
    "\n",
    "        if not all_results:\n",
    "            return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n",
    "        return pd.DataFrame(all_results)[['source_id', 'sink_id', 'co2_transported', 'path']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Iterative optimization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame(columns=['source_id', 'sink_id', 'co2_transported', 'path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example Data Preparation\n",
    "def prepare_example_data():\n",
    "    # Example Sources (Cement Plants)\n",
    "    source_data = {\n",
    "        'id': ['1897479', '1897478', '32438922', '1897504', '32438914'],\n",
    "        'name': ['CEMEX Mexico SA de CV Puebla 1', \n",
    "                'CEMEX Mexico SA de CV Hidalgo 1',\n",
    "                'Holcim Mexico SA de CV Veracruz 1',\n",
    "                'Holcim Mexico SA de CV Tabasco 1',\n",
    "                'Cementos Fortaleza Hidalgo 2'],\n",
    "        'emission': [2.794440e+06, 1.036068e+06, 9.600968e+05, 8.838254e+05, 8.025427e+05],\n",
    "        'lat': [18.967968, 19.996589, 18.857600, 17.651499, 20.011934],\n",
    "        'lon': [-97.961858, -99.210844, -97.041208, -92.443296, -99.178466]\n",
    "    }\n",
    "    df_source = pd.DataFrame(source_data)\n",
    "\n",
    "    # Example Sinks (Storage Sites)\n",
    "    sink_data = {\n",
    "        'id': ['10346', '10347', '10348', '10349', '10350'],\n",
    "        'sum_mid': [4.300000e+08, 1.040000e+09, 1.920000e+09, 1.970000e+09, 5.400000e+08],\n",
    "        'latitude': [25.642778, 25.664722, 25.758611, 25.779444, 25.809444],\n",
    "        'longitude': [-99.071389, -98.954444, -98.580000, -98.438611, -97.863611],\n",
    "        'site_name': ['Burgos B1A', 'Burgos B1B', 'Burgos B1C', 'Burgos B1D', 'Burgos B1E']\n",
    "    }\n",
    "    df_sink = pd.DataFrame(sink_data)\n",
    "\n",
    "    # Example Cost Matrix (Distances between sources and sinks)\n",
    "    cost_matrix_data = {\n",
    "        'Unnamed: 0': ['10346', '10347', '10348', '10349', '10350'],\n",
    "        '1897479': [345.817296, 344.745828, 348.487164, 340.211268, 336.485808],\n",
    "        '1897478': [332.455428, 329.412024, 333.153360, 324.877428, 321.152004],\n",
    "        '32438922': [373.947948, 372.876480, 376.617816, 368.341884, 364.616460],\n",
    "        '1897504': [521.441244, 520.369740, 524.111148, 515.835180, 512.109720],\n",
    "        '32438914': [331.633836, 329.152284, 332.893620, 324.617688, 320.892264]\n",
    "    }\n",
    "    df_cost_matrix = pd.DataFrame(cost_matrix_data)\n",
    "\n",
    "    return df_source, df_sink, df_cost_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [source_id, sink_id, co2_transported, path]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Prepare data (with string IDs)\n",
    "df_source, df_sink, df_cost_matrix = prepare_example_data()\n",
    "\n",
    "# Run optimization\n",
    "results = iterative_clustered_network_optimization(\n",
    "    df_source=df_source,\n",
    "    df_sink=df_sink,\n",
    "    df_cost_matrix=df_cost_matrix,\n",
    "    source_id='id',\n",
    "    sink_id='id',\n",
    "    source_capacity='emission',\n",
    "    sink_capacity='sum_mid',\n",
    "    emission_cost=50,\n",
    "    transport_method='pipe',\n",
    "    quantity_cost_segments=quantity_cost_segments,\n",
    "    capture_cost=30,\n",
    "    stock_cost=10,\n",
    "    max_iter=5\n",
    ")\n",
    "\n",
    "print(results[['source_id', 'sink_id', 'co2_transported', 'path']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
